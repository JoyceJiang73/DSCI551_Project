{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape the list of political subReddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.reddit.com/r/redditlists/comments/josdr/list_of_political_subreddits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>List of political subreddits. : redditlists</title>\n",
      "World News\n",
      "http://www.reddit.com/r/worldnews\n",
      "News\n",
      "http://www.reddit.com/r/news\n",
      "World Politics\n",
      "http://www.reddit.com/r/worldpolitics\n",
      "World Events\n",
      "http://www.reddit.com/r/Worldevents\n",
      "Business\n",
      "http://reddit.com/r/business\n",
      "Economics\n",
      "http://reddit.com/r/Economics\n",
      "Environment\n",
      "http://reddit.com/r/environment\n",
      "Energy\n",
      "http://www.reddit.com/r/energy\n",
      "Law\n",
      "http://reddit.com/r/law/\n",
      "Education\n",
      "http://www.reddit.com/r/education/\n",
      "Government\n",
      "http://www.reddit.com/r/government/\n",
      "History\n",
      "http://www.reddit.com/r/history/\n",
      "Politics PDF's\n",
      "http://www.reddit.com/r/PoliticsPDFs/\n",
      "Wikileaks\n",
      "http://www.reddit.com/r/WikiLeaks/\n",
      "SOPA\n",
      "http://www.reddit.com/r/SOPA/\n",
      "News Porn\n",
      "http://www.reddit.com/r/NewsPorn\n",
      "World News 2\n",
      "http://www.reddit.com/r/worldnews2/\n",
      "Anarchist News\n",
      "http://www.reddit.com/r/AnarchistNews/\n",
      "Republic of Politics\n",
      "http://www.reddit.com/r/republicofpolitics\n",
      "LGBT News\n",
      "http://www.reddit.com/r/LGBTnews/\n",
      "Politics 2\n",
      "http://www.reddit.com/r/politics2/\n",
      "Economics 2\n",
      "http://www.reddit.com/r/economics2/\n",
      "Environment 2\n",
      "http://www.reddit.com/r/environment2/\n",
      "Politics\n",
      "http://www.reddit.com/r/politics\n",
      "US Politics\n",
      "http://www.reddit.com/r/uspolitics\n",
      "American Politics\n",
      "http://reddit.com/r/AmericanPolitics\n",
      "Canada\n",
      "http://www.reddit.com/r/canada\n",
      "American Government\n",
      "http://reddit.com/r/AmericanGovernment\n",
      "UK Politics\n",
      "http://reddit.com/r/ukpolitics\n",
      "Euro\n",
      "http://www.reddit.com/r/euro\n",
      "Palestine\n",
      "http://www.reddit.com/r/Palestine/\n",
      "EU Politics\n",
      "http://www.reddit.com/r/eupolitics\n",
      "Middle East News\n",
      "http://www.reddit.com/r/MiddleEastNews\n",
      "Israel\n",
      "http://www.reddit.com/r/Israel/\n",
      "India\n",
      "http://www.reddit.com/r/india\n",
      "Pakistan\n",
      "http://www.reddit.com/r/pakistan\n",
      "Cascadia\n",
      "http://www.reddit.com/r/Cascadia\n",
      "Iran\n",
      "http://www.reddit.com/r/iran\n",
      "Wisconsin\n",
      "http://www.reddit.com/r/wisconsinpolitics\n",
      "Libertarian\n",
      "http://reddit.com/r/Libertarian\n",
      "Anarchism\n",
      "http://reddit.com/r/Anarchism\n",
      "Socialism\n",
      "http://reddit.com/r/socialism\n",
      "Progressive\n",
      "http://reddit.com/r/progressive\n",
      "Conservative\n",
      "http://reddit.com/r/Conservative\n",
      "American Pirate Party\n",
      "http://reddit.com/r/americanpirateparty\n",
      "Democrats\n",
      "http://reddit.com/r/democrats\n",
      "Liberal\n",
      "http://reddit.com/r/Liberal\n",
      "New Right\n",
      "http://reddit.com/r/new_right\n",
      "Republican\n",
      "http://reddit.com/r/Republican\n",
      "Egalitarian\n",
      "http://www.reddit.com/r/egalitarian\n",
      "Democratic Socialism\n",
      "http://www.reddit.com/r/demsocialist\n",
      "Libertarian Left\n",
      "http://www.reddit.com/r/LibertarianLeft\n",
      "Liberty\n",
      "http://www.reddit.com/r/Liberty\n",
      "Anarcho Capitalism\n",
      "http://www.reddit.com/r/Anarcho_Capitalism\n",
      "All the Left\n",
      "http://www.reddit.com/r/alltheleft\n",
      "Neo-Progressives\n",
      "http://www.reddit.com/r/neoprogs\n",
      "Tea Party\n",
      "http://www.reddit.com/r/tea_party\n",
      "Voluntarism\n",
      "http://www.reddit.com/r/voluntarism/\n",
      "Labor\n",
      "http://reddit.com/r/labor\n",
      "Black Flag\n",
      "http://www.reddit.com/r/blackflag\n",
      "Green Party\n",
      "http://www.reddit.com/r/GreenParty\n",
      "Democracy\n",
      "http://www.reddit.com/r/democracy\n",
      "International Workers of the World\n",
      "http://www.reddit.com/r/IWW\n",
      "Pirate Party\n",
      "http://www.reddit.com/r/PirateParty\n",
      "Marxism\n",
      "http://www.reddit.com/r/Marxism\n",
      "Piratenpartei Deutschland\n",
      "http://www.reddit.com/r/piratenpartei\n",
      "Objectivism\n",
      "http://www.reddit.com/r/Objectivism\n",
      "Libertarian Socialism\n",
      "http://www.reddit.com/r/LibertarianSocialism\n",
      "People's Party\n",
      "http://www.reddit.com/r/peoplesparty\n",
      "Capitalism\n",
      "http://www.reddit.com/r/Capitalism\n",
      "Anarchist\n",
      "http://www.reddit.com/r/Anarchist\n",
      "Feminisms\n",
      "http://www.reddit.com/r/feminisms\n",
      "Republicans\n",
      "http://www.reddit.com/r/republicans\n",
      "Egalitarianism\n",
      "http://www.reddit.com/r/Egalitarianism\n",
      "AnarchaFeminism\n",
      "http://www.reddit.com/r/anarchafeminism\n",
      "Communist\n",
      "http://www.reddit.com/r/Communist\n",
      "Social Democracy\n",
      "http://www.reddit.com/r/socialdemocracy\n",
      "Post Left Anarchism\n",
      "http://www.reddit.com/r/Postleftanarchism\n",
      "Radical Feminism\n",
      "http://www.reddit.com/r/RadicalFeminism/\n",
      "Anarcho Pacifism\n",
      "http://www.reddit.com/r/AnarchoPacifism\n",
      "Conservatives\n",
      "http://www.reddit.com/r/conservatives/\n",
      "Republicanism\n",
      "http://reddit.com/r/republicanism\n",
      "Free Thought\n",
      "http://www.reddit.com/r/Freethought\n",
      "Food for Thought\n",
      "http://www.reddit.com/r/Foodforthought\n",
      "State of the Union\n",
      "http://www.reddit.com/r/StateOfTheUnion/\n",
      "Moderate Politics\n",
      "http://reddit.com/r/moderatepolitics\n",
      "Political Discussion\n",
      "http://www.reddit.com/r/PoliticalDiscussion\n",
      "Equality\n",
      "http://www.reddit.com/r/equality\n",
      "Cultural Studies\n",
      "http://reddit.com/r/culturalstudies\n",
      "Political Humor\n",
      "http://www.reddit.com/r/politicalhumor\n",
      "Propaganda Posters\n",
      "http://reddit.com/r/propagandaposters\n",
      "Social Science\n",
      "http://www.reddit.com/r/SocialScience/\n",
      "Political Philosophy\n",
      "http://www.reddit.com/r/PoliticalPhilosophy\n",
      "Media\n",
      "http://www.reddit.com/r/media\n",
      "Culture\n",
      "http://www.reddit.com/r/culture\n",
      "Racism\n",
      "http://www.reddit.com/r/racism\n",
      "Corruption\n",
      "http://www.reddit.com/r/corruption\n",
      "Intellectual Property Rights\n",
      "http://reddit.com/r/ipr\n",
      "Noam Chomsky\n",
      "http://www.reddit.com/r/chomsky\n",
      "Propaganda\n",
      "http://reddit.com/r/propaganda\n",
      "Peter Schiff\n",
      "http://www.reddit.com/r/PeterSchiff\n",
      "Voting Theory\n",
      "http://www.reddit.com/r/votingtheory\n",
      "Religion In America\n",
      "http://www.reddit.com/r/ReligionInAmerica/\n",
      "Economics Papers\n",
      "http://www.reddit.com/r/EconPapers\n",
      "Debate\n",
      "http://www.reddit.com/r/debate\n",
      "Food Sovereignty\n",
      "http://www.reddit.com/r/FoodSovereignty/\n",
      "Environmental Policy\n",
      "http://www.reddit.com/r/environmental_policy\n",
      "LGBT\n",
      "http://www.reddit.com/r/lgbt/\n",
      "Men's Rights\n",
      "http://www.reddit.com/r/MensRights\n",
      "Collapse\n",
      "http://www.reddit.com/r/collapse\n",
      "Operation Grab Ass\n",
      "http://www.reddit.com/r/OperationGrabAss\n",
      "Hack Bloc\n",
      "http://www.reddit.com/r/HackBloc\n",
      "RPAC - The Open Source Democracy Foundation\n",
      "http://www.reddit.com/r/rpac\n",
      "Bad Cop No Donut\n",
      "http://www.reddit.com/r/Bad_Cop_No_Donut/\n",
      "Anti-Consumption\n",
      "http://www.reddit.com/r/Anticonsumption\n",
      "Permaculture\n",
      "http://www.reddit.com/r/Permaculture/\n",
      "Food 2\n",
      "http://www.reddit.com/r/food2\n",
      "Anonymous\n",
      "http://reddit.com/r/anonymous\n",
      "Censorship\n",
      "http://www.reddit.com/r/censorship\n",
      "Feminism\n",
      "http://www.reddit.com/r/feminism\n",
      "Sunlight\n",
      "http://www.reddit.com/r/Sunlight\n",
      "Privacy\n",
      "http://www.reddit.com/r/privacy/\n",
      "Occupy Wall Street\n",
      "http://www.reddit.com/r/occupywallstreet\n",
      "Resilient Communities\n",
      "http://www.reddit.com/r/resilientcommunities\n",
      "Change Now\n",
      "http://www.reddit.com/r/ChangeNow\n",
      "Reddit Political Activism\n",
      "http://www.reddit.com/r/rpa\n",
      "Gender Egaliatarian\n",
      "http://www.reddit.com/r/GenderEgalitarian\n",
      "Activism\n",
      "http://www.reddit.com/r/activism\n",
      "Revolution\n",
      "http://www.reddit.com/r/revolution\n",
      "Nazi Hunting\n",
      "http://www.reddit.com/r/NaziHunting\n",
      "Prison Reform\n",
      "http://www.reddit.com/r/prisonreform\n",
      "TSA\n",
      "http://www.reddit.com/r/tsa\n",
      "Election Reform\n",
      "http://www.reddit.com/r/electionreform\n",
      "Troubled Teens\n",
      "http://www.reddit.com/r/troubledteens\n",
      "First Amendment\n",
      "http://www.reddit.com/r/firstamendment\n",
      "Sensible Washington\n",
      "http://www.reddit.com/r/sensiblewashington/\n",
      "The War on Drugs\n",
      "http://www.reddit.com/r/Thewarondrugs/\n",
      "Union\n",
      "http://www.reddit.com/r/union/\n",
      "Good Cop Free Donut\n",
      "http://www.reddit.com/r/Good_Cop_Free_Donut\n",
      "Strike Action\n",
      "http://www.reddit.com/r/strikeaction/\n",
      "Youth Rights\n",
      "http://www.reddit.com/r/YouthRights\n",
      "Phx Class War Council\n",
      "http://www.reddit.com/r/PhxClassWarCouncil\n",
      "Human Rights\n",
      "http://www.reddit.com/r/humanrights\n",
      "CPAR\n",
      "http://www.reddit.com/r/CPAR/\n",
      "White Rights\n",
      "http://www.reddit.com/r/whiterights\n",
      "Obama\n",
      "http://reddit.com/r/obama\n",
      "Ron Paul\n",
      "http://reddit.com/r/ronpaul\n",
      "Kucinich\n",
      "http://www.reddit.com/r/Kucinich\n",
      "Palin Problem\n",
      "http://reddit.com/r/PalinProblem\n",
      "Gary Johnson\n",
      "http://www.reddit.com/r/GaryJohnson\n",
      "Bachmann 2012\n",
      "http://www.reddit.com/r/Bachmann2012/\n",
      "United States Presidential Election 2012\n",
      "http://www.reddit.com/r/USPE12\n",
      "2012 Elections\n",
      "http://www.reddit.com/r/2012Elections\n",
      "Campaigns\n",
      "http://www.reddit.com/r/Campaigns\n",
      "Perry 2012\n",
      "http://www.reddit.com/r/perry2012\n",
      "Huntsman\n",
      "http://www.reddit.com/r/huntsman\n",
      "Huntsman 2012\n",
      "http://www.reddit.com/r/huntsman2012\n",
      "Newt 2012\n",
      "http://www.reddit.com/r/newt2012\n",
      "Romney\n",
      "http://www.reddit.com/r/romney\n",
      "Black Ops\n",
      "http://www.reddit.com/r/BlackOps\n",
      "Intelligence\n",
      "http://www.reddit.com/r/Intelligence\n",
      "MidEast Peace\n",
      "http://reddit.com/r/MideastPeace\n",
      "Endless War\n",
      "http://reddit.com/r/EndlessWar\n",
      "Antiwar\n",
      "http://reddit.com/r/antiwar\n",
      "War\n",
      "http://www.reddit.com/r/war\n",
      "Peace\n",
      "http://www.reddit.com/r/peace\n",
      "Afghanistan\n",
      "http://www.reddit.com/r/afghanistan/\n",
      "Libya\n",
      "http://www.reddit.com/r/Libya\n",
      "Conspiracy\n",
      "http://www.reddit.com/r/conspiracy\n",
      "9/11 Truth\n",
      "http://www.reddit.com/r/911truth\n",
      "Climate Skeptics\n",
      "http://www.reddit.com/r/climateskeptics\n",
      "Info Graffiti\n",
      "http://www.reddit.com/r/infograffiti\n",
      "Conspiracy Hub\n",
      "http://www.reddit.com/r/conspiracyhub\n",
      "Redditors for 9/11 Truth\n",
      "http://www.reddit.com/r/redditorsfor911truth\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "req = Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "webpage = urlopen(req).read()\n",
    "page_soup = soup(webpage, \"html.parser\")\n",
    "title = page_soup.find(\"title\")\n",
    "print(title)\n",
    "containers = page_soup.findAll('a',class_=\"_3t5uN8xUmg0TOwRCOGQEcU\")\n",
    "\n",
    "names=[]\n",
    "urls=[]\n",
    "pairs=[]\n",
    "\n",
    "for container in containers[:174]:\n",
    "    names.append(container.text)\n",
    "    urls.append(container['href'])\n",
    "    pairs.append((container.text,container['href']))\n",
    "    print(container.text)\n",
    "    print(container['href'])\n",
    "\n",
    "subreddit=pd.DataFrame(pairs,columns=['name','url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit.to_csv('Data/subReddit_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reddit Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('fQ-VABpboAY8iQ', 'v6X0ifFGgMRop3g2TlbV8cyX0LXxOQ')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': 'JoyceOoops',\n",
    "        'password': '19960703jyr'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'MyBot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "# Add headers=headers to every request\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id = 'fQ-VABpboAY8iQ', \n",
    "                     client_secret = 'v6X0ifFGgMRop3g2TlbV8cyX0LXxOQ', \n",
    "                     user_agent = 'JoyceOoops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "received 403 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1c9d2da9df30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    \"search_type\":[]}\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtopics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtopics_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/models/listing/generator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/models/listing/generator.py\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# for submission duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objectify_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     def info(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBadRequest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    258\u001b[0m             )\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: received 403 HTTP response"
     ]
    }
   ],
   "source": [
    "for x in range(len(names))[17:]:\n",
    "    \n",
    "    subreddit = reddit.subreddit(urls[x].split('/')[4]) \n",
    "    posts = subreddit.hot(limit=1000)\n",
    "\n",
    "    topics_dict = { \"subreddit\":[],\n",
    "                   \"title\":[],\n",
    "                   \"score\":[], \n",
    "                   \"id\":[], \n",
    "                   \"url\":[], \n",
    "                   \"comms_num\": [], \n",
    "                   \"created\": [], \n",
    "                   \"body\":[],\n",
    "                   \"search_type\":[]}\n",
    "\n",
    "    for submission in posts:\n",
    "        topics_dict[\"subreddit\"].append(names[x])\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"title\"].append(submission.title)\n",
    "        topics_dict[\"score\"].append(submission.score)\n",
    "        topics_dict[\"url\"].append(submission.url)\n",
    "        topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "        topics_dict[\"created\"].append(datetime.datetime.fromtimestamp(submission.created))\n",
    "    #datetime.datetime.fromtimestamp(submission.created)\n",
    "        topics_dict[\"body\"].append(submission.selftext)\n",
    "        topics_dict[\"search_type\"].append(\"new\")\n",
    "\n",
    "    topics_data = pd.DataFrame(topics_dict)\n",
    "    topics_data.head()\n",
    "\n",
    "    topics_data.to_csv(\"Data/PRAW/PRAW_\"+names[x]+\".csv\")\n",
    "    print(x,names[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAW_Business.csv\n",
      "PRAW_World News.csv\n",
      "PRAW_Energy.csv\n",
      "PRAW_World Events.csv\n",
      "PRAW_World News 2.csv\n",
      "PRAW_News.csv\n",
      "PRAW_History.csv\n",
      "PRAW_Economics.csv\n",
      "PRAW_Politics PDF's.csv\n",
      "PRAW_Environment.csv\n",
      "PRAW.csv\n",
      "PRAW_Government.csv\n",
      "PRAW_Law.csv\n",
      "PRAW_Wikileaks.csv\n",
      "PRAW_News Porn.csv\n",
      "PRAW_Education.csv\n",
      "PRAW_World Politics.csv\n",
      "PRAW_SOPA.csv\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "docs_location = \"Data/PRAW/\"\n",
    "\n",
    "for doc in os.listdir(docs_location):\n",
    "    print(doc)\n",
    "    df = pd.read_csv(os.path.join(docs_location + doc))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>search_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Business</td>\n",
       "      <td>Posts regarding politics</td>\n",
       "      <td>81</td>\n",
       "      <td>kurvl4</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/kur...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-11 01:15:49</td>\n",
       "      <td>Many of you know, we have a strict no-politics...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "      <td>USDOT report shows autonomous trucks will only...</td>\n",
       "      <td>271</td>\n",
       "      <td>m8izf6</td>\n",
       "      <td>https://www.traffictechnologytoday.com/news/au...</td>\n",
       "      <td>58</td>\n",
       "      <td>2021-03-19 15:54:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "      <td>NFL announces new TV deals, with Amazon gettin...</td>\n",
       "      <td>314</td>\n",
       "      <td>m8ganl</td>\n",
       "      <td>https://www.washingtonpost.com/sports/2021/03/...</td>\n",
       "      <td>67</td>\n",
       "      <td>2021-03-19 13:37:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Ford to hold some F-150s without certain modul...</td>\n",
       "      <td>197</td>\n",
       "      <td>m8j14u</td>\n",
       "      <td>https://www.autonews.com/manufacturing/ford-ho...</td>\n",
       "      <td>39</td>\n",
       "      <td>2021-03-19 15:56:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Business</td>\n",
       "      <td>Lyft says it just had the most riders in a sin...</td>\n",
       "      <td>625</td>\n",
       "      <td>m7zhs6</td>\n",
       "      <td>https://www.cnbc.com/2021/03/18/lyft-says-it-j...</td>\n",
       "      <td>25</td>\n",
       "      <td>2021-03-18 21:09:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 subreddit                                              title  \\\n",
       "0           0  Business                           Posts regarding politics   \n",
       "1           1  Business  USDOT report shows autonomous trucks will only...   \n",
       "2           2  Business  NFL announces new TV deals, with Amazon gettin...   \n",
       "3           3  Business  Ford to hold some F-150s without certain modul...   \n",
       "4           4  Business  Lyft says it just had the most riders in a sin...   \n",
       "\n",
       "   score      id                                                url  \\\n",
       "0     81  kurvl4  https://www.reddit.com/r/business/comments/kur...   \n",
       "1    271  m8izf6  https://www.traffictechnologytoday.com/news/au...   \n",
       "2    314  m8ganl  https://www.washingtonpost.com/sports/2021/03/...   \n",
       "3    197  m8j14u  https://www.autonews.com/manufacturing/ford-ho...   \n",
       "4    625  m7zhs6  https://www.cnbc.com/2021/03/18/lyft-says-it-j...   \n",
       "\n",
       "   comms_num              created  \\\n",
       "0          0  2021-01-11 01:15:49   \n",
       "1         58  2021-03-19 15:54:04   \n",
       "2         67  2021-03-19 13:37:02   \n",
       "3         39  2021-03-19 15:56:21   \n",
       "4         25  2021-03-18 21:09:32   \n",
       "\n",
       "                                                body search_type  \n",
       "0  Many of you know, we have a strict no-politics...         new  \n",
       "1                                                NaN         new  \n",
       "2                                                NaN         new  \n",
       "3                                                NaN         new  \n",
       "4                                                NaN         new  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat(dfs, sort=False)\n",
    "df=df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data/PRAW_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second method of scraping Reddit posts using Reddit Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response(res):\n",
    "    # initialize dataframe\n",
    "    df = pd.DataFrame()\n",
    "    images=[]\n",
    "\n",
    "# loop through each post retrieved from GET request\n",
    "    for post in res.json()['data']['children']:\n",
    "        # append relevant data to dataframe\n",
    "        df = df.append({\n",
    "            'id': post['data']['id'],\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'kind':post['kind'],\n",
    "            'create_time':post['data']['created']\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "        try:\n",
    "            string=post['data']['preview']['images'][0]['source']['url']\n",
    "            string=string.replace('amp;', '')\n",
    "            images.append(string)\n",
    "        except KeyError:\n",
    "            images.append(np.nan)\n",
    "        \n",
    "    df['image']=images\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-ed786926a9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# get dataframe from response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# take the final row (oldest entry)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-2ce5c2dfcfc3>\u001b[0m in \u001b[0;36mdf_from_response\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# loop through each post retrieved from GET request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# append relevant data to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         df = df.append({\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "for x in range(len(names)):\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': 100}\n",
    "\n",
    "    for i in range(9):\n",
    "    # make request\n",
    "        res = requests.get(urls[x],\n",
    "                           headers=headers,\n",
    "                           params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response(res)\n",
    "        # take the final row (oldest entry)\n",
    "        row = new_df.iloc[len(new_df)-1]\n",
    "        # create fullname\n",
    "        fullname = row['kind'] + '_' + row['id']\n",
    "        # add/update fullname in params\n",
    "        params['after'] = fullname\n",
    "    \n",
    "        # append new_df to data\n",
    "        data = data.append(new_df, ignore_index=True)\n",
    "        print(i)\n",
    "    \n",
    "    data.to_csv(\"Data/RedditApi/RedditApi_\"+names[x]+\".csv\")\n",
    "    print(x,names[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.reddit.com/r/worldnews'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>create_time</th>\n",
       "      <th>downs</th>\n",
       "      <th>id</th>\n",
       "      <th>kind</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.616234e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m8wtda</td>\n",
       "      <td>t3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hey Guys,\\n\\nWhy is it that some people when u...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Kmeans using Sklearn in Python Question</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.616233e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m8wkwu</td>\n",
       "      <td>t3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hello everyone,\\n\\nI'm new to this subreddit. ...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Monocular Depth Estimation - Running multiple ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://external-preview.redd.it/RTriw7tx8Z-9y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.616230e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m8vqw7</td>\n",
       "      <td>t3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I now know to automate few repetitive steps us...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Hosting a python script</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.616228e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m8v0ax</td>\n",
       "      <td>t3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Its not pretty to look at, but it works..  \\[*...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Wiki gTTS from the terminal - 1 night project</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.616228e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>m8uv8d</td>\n",
       "      <td>t3</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "      <td>Writing a Binary Search Tree in Python - With ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://external-preview.redd.it/Wv7W27i7w0Tqa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>1.614135e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lqps5m</td>\n",
       "      <td>t3</td>\n",
       "      <td>46.0</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "      <td>Python Developers Survey 2020 Results</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>https://external-preview.redd.it/PPh_JNhM8TJ-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1.614134e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lqpla8</td>\n",
       "      <td>t3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>I will soon start my master thesis and it will...</td>\n",
       "      <td>Python</td>\n",
       "      <td>Transition to python from MATLAB</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>1.614133e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lqp6fu</td>\n",
       "      <td>t3</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "      <td>[1 day left] Full Stack Web Application Develo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>https://external-preview.redd.it/2YMaaCsqcKQic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>1.614133e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lqp43m</td>\n",
       "      <td>t3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>What \"design of experiments\" package to go wit...</td>\n",
       "      <td>Python</td>\n",
       "      <td>What \"design of experiments\" package to go wit...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>1.614131e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>lqocuc</td>\n",
       "      <td>t3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>After asking the particular question myself, \"...</td>\n",
       "      <td>Python</td>\n",
       "      <td>An easy way to deal with __pycache__ bytecode ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      create_time  downs      id kind  score  \\\n",
       "0    1.616234e+09    0.0  m8wtda   t3    1.0   \n",
       "1    1.616233e+09    0.0  m8wkwu   t3    2.0   \n",
       "2    1.616230e+09    0.0  m8vqw7   t3    2.0   \n",
       "3    1.616228e+09    0.0  m8v0ax   t3    1.0   \n",
       "4    1.616228e+09    0.0  m8uv8d   t3    2.0   \n",
       "..            ...    ...     ...  ...    ...   \n",
       "795  1.614135e+09    0.0  lqps5m   t3   46.0   \n",
       "796  1.614134e+09    0.0  lqpla8   t3    9.0   \n",
       "797  1.614133e+09    0.0  lqp6fu   t3    0.0   \n",
       "798  1.614133e+09    0.0  lqp43m   t3    2.0   \n",
       "799  1.614131e+09    0.0  lqocuc   t3    0.0   \n",
       "\n",
       "                                              selftext subreddit  \\\n",
       "0    Hey Guys,\\n\\nWhy is it that some people when u...    Python   \n",
       "1    Hello everyone,\\n\\nI'm new to this subreddit. ...    Python   \n",
       "2    I now know to automate few repetitive steps us...    Python   \n",
       "3    Its not pretty to look at, but it works..  \\[*...    Python   \n",
       "4                                                         Python   \n",
       "..                                                 ...       ...   \n",
       "795                                                       Python   \n",
       "796  I will soon start my master thesis and it will...    Python   \n",
       "797                                                       Python   \n",
       "798  What \"design of experiments\" package to go wit...    Python   \n",
       "799  After asking the particular question myself, \"...    Python   \n",
       "\n",
       "                                                 title   ups  upvote_ratio  \\\n",
       "0              Kmeans using Sklearn in Python Question   1.0          1.00   \n",
       "1    Monocular Depth Estimation - Running multiple ...   2.0          1.00   \n",
       "2                              Hosting a python script   2.0          0.75   \n",
       "3        Wiki gTTS from the terminal - 1 night project   1.0          1.00   \n",
       "4    Writing a Binary Search Tree in Python - With ...   2.0          1.00   \n",
       "..                                                 ...   ...           ...   \n",
       "795              Python Developers Survey 2020 Results  46.0          0.94   \n",
       "796                   Transition to python from MATLAB   9.0          0.80   \n",
       "797  [1 day left] Full Stack Web Application Develo...   0.0          0.29   \n",
       "798  What \"design of experiments\" package to go wit...   2.0          1.00   \n",
       "799  An easy way to deal with __pycache__ bytecode ...   0.0          0.33   \n",
       "\n",
       "                                                 image  \n",
       "0                                                  NaN  \n",
       "1    https://external-preview.redd.it/RTriw7tx8Z-9y...  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4    https://external-preview.redd.it/Wv7W27i7w0Tqa...  \n",
       "..                                                 ...  \n",
       "795  https://external-preview.redd.it/PPh_JNhM8TJ-v...  \n",
       "796                                                NaN  \n",
       "797  https://external-preview.redd.it/2YMaaCsqcKQic...  \n",
       "798                                                NaN  \n",
       "799                                                NaN  \n",
       "\n",
       "[800 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "for x in range(len(df.title)):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(df['image'][x], \"images/\"+str(x)+\".jpg\")\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "entities=[]\n",
    "\n",
    "for x in df['title']:\n",
    "    doc = nlp(x)\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text,ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_lists=[]\n",
    "\n",
    "for r in range(len(df.title)):\n",
    "    doc = nlp(df.title[r])\n",
    "    for ent in doc.ents:\n",
    "        NER_lists.append((ent.text, ent.start_char, ent.end_char, ent.label_,df.title[r],df.id[r]))\n",
    "\n",
    "NER=pd.DataFrame(NER_lists,columns=['TEXT','START','END','LABEL','Sentence','Article'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER.to_csv('Data/NER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Reddit_SampleData2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
